```{python}
with open("cuban.txt", "r", encoding="utf-8") as f:
    cuban_text = f.read()
```

```{python}
import re
cuban_words = re.split(r"[\s\W]+", cuban_text)
cuban_words = [w for w in cuban_words if w]
cuban_words = [w.lower() for w in cuban_words]
cuban_words = re.findall(r"[a-záéíóúüñ]+", cuban_text)

```

```{python}
import pandas as pd
import re
from collections import Counter
import spacy

# Load Spanish spaCy model
nlp = spacy.load("es_core_news_sm")

# ---------------------------------------------------
# 0. ASSUMING cuban_words IS ALREADY LOADED
# cuban_words should be a list of word tokens
# Example:
# cuban_words = ["pescao","feo","melao",...]
# ---------------------------------------------------


# -----------------------------
# 1. Basic frequency function
# -----------------------------
def word_freq(words):
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    return freq

cuban_freq = word_freq(cuban_words)


# -------------------------------------
# 2. Words of interest (given by instructor)
# -------------------------------------
words = [
    'que', 'qué', 'los', 'loh', 'lo',
    'bola', 'vola', 'volá', 'bolá',
    'asere', 'acere',
    'pescao', 'pescado',
    'mantecao', 'mantecado',
    'cuidao', 'cuidado',
    'melao', 'cansado', 
    'complicado', 'complicao', 'picado', 'picao', 'castro', 'fidel',
    'estao', 'pesao', 'combinao', 'kagao', 'pegao', 'pasao', 'apretao', 'cacao', 'pelao', 'ansao', 'cuidaos', 'guardaos', 'ocupaos', 'estaos', 'reparaos', 'mataos', 'olvidaos'
]

freq_data = {w: cuban_freq.get(w, 0) for w in words}


# ---------------------------------------------------
# 3. AUTOMATIC SOUND-CHANGE PATTERN EXTRACTION
# ---------------------------------------------------
ao_forms   = [w for w in cuban_words if re.search(r'ao$', w)]
ado_forms   = [w for w in cuban_words if re.search(r'ado$', w)]
aa_forms   = [w for w in cuban_words if re.search(r'aa$', w)]
ada_forms   = [w for w in cuban_words if re.search(r'ada$', w)]
aos_forms  = [w for w in cuban_words if re.search(r'aos$', w)]
ados_forms   = [w for w in cuban_words if re.search(r'ados$', w)]
aas_forms  = [w for w in cuban_words if re.search(r'aas$', w)]
adas_forms   = [w for w in cuban_words if re.search(r'adas$', w)]

final_h_forms = [w for w in cuban_words if re.search(r'h$', w) and not re.search(r'ch$', w)]

b_initial_forms = [w for w in cuban_words if re.search(r'^b', w)]
v_initial_forms = [w for w in cuban_words if re.search(r'^v', w)]


# ---------------------------------------------------
# 4. BUILD INITIAL DATAFRAME
# ---------------------------------------------------
df = pd.DataFrame(list(freq_data.items()), columns=['word', 'frequency'])

df['length'] = df['word'].apply(len)
df['has_accent'] = df['word'].apply(lambda x: bool(re.search(r'[áéíóú]', x)))

# Multiple sound-change flags
df['ado_to_ao_shift'] = df['word'].apply(lambda w: 1 if re.search(r'ao$', w) else 0)
df['ada_to_aa_shift'] = df['word'].apply(lambda w: 1 if re.search(r'aa$', w) else 0)
df['plural_aos_shift'] = df['word'].apply(lambda w: 1 if re.search(r'aos$', w) else 0)
df['plural_aas_shift'] = df['word'].apply(lambda w: 1 if re.search(r'aas$', w) else 0)
df['final_s_to_h']     = df['word'].apply(lambda w: 1 if re.search(r'h$', w) else 0)
df['v_to_b_shift']     = df['word'].apply(lambda w: 1 if re.search(r'^b', w) else 0)


# ---------------------------------------------------
# 5. LEMMATIZATION HELPER
# ---------------------------------------------------
def get_spacy_lemma(word):
    doc = nlp(word)
    return doc[0].lemma_

df["spacy_lemma"] = df["word"].apply(get_spacy_lemma)


# ---------------------------------------------------
# 6. RULE-BASED RECOVERY OF STANDARD FORM
# ---------------------------------------------------
def recover_standard_form(word):
    if re.search(r'ao$', word):  return re.sub(r'ao$', 'ado', word)
    if re.search(r'aos$', word): return re.sub(r'aos$', 'ados', word)
    if re.search(r'aa$', word):  return re.sub(r'aa$', 'ada', word)
    if re.search(r'aas$', word): return re.sub(r'aas$', 'adas', word)
    if re.search(r'h$', word) and not re.search(r'ch$', word): return re.sub(r'h$', 's', word)
    if re.search(r'^b', word): return 'v' + word[1:]
    return word

df["lemma_after_rules"] = df["word"].apply(recover_standard_form)
df["lemma_recovered_spacy"] = df["lemma_after_rules"].apply(get_spacy_lemma)
df["is_recovered"] = df.apply(lambda row: row["spacy_lemma"] == row["lemma_recovered_spacy"], axis=1)


# ---------------------------------------------------
# 7. NEW: POS TAGGING FOR -ADO / -AO WORDS
# ---------------------------------------------------
def get_pos(word):
    doc = nlp(word)
    return doc[0].pos_   # spaCy POS tag (ADJ, NOUN, VERB, etc.)

df["pos_tag"] = df["word"].apply(get_pos)  # POS tag of original Cuban form
df["pos_standard"] = df["lemma_after_rules"].apply(get_pos)  # POS tag of standard form


# ---------------------------------------------------
# 8. Sort + Export to CSV
# ---------------------------------------------------
df = df.sort_values(by='frequency', ascending=False)
df.to_csv("lopez_castano_melanie_component2.csv", index=False, encoding="utf-8")
print(ao_forms)
print(ado_forms)
print(df)
```

### **Project Summary**

**Raw file:** The raw data came from *El Corpus del Español* (<https://www.corpusdelespanol.org/>), specifically from a subcorpus of Cuban Spanish texts. I downloaded a plain text file containing word tokens from conversational or informal Cuban speech. I chose a variety of texts ranging from political commentary to social media postings to get a wide range of language use.

**Cleaning steps:** I cleaned the text by converting everything to lowercase, removing punctuation, trimming extra whitespace, and tokenizing it into individual words. This allowed for consistent frequency counts and pattern matching (e.g., words ending in *-ao* vs. *-ado*). I kept accent marks and distinguished items like "qué" and "que"

**Output CSV:**\

The final CSV contains the following columns:

-   **word** – the token being analyzed

-   **frequency** – number of times it appears in the corpus

-   **length** – character count of each word

-   **has_accent** – marks whether the word includes an accent mark

-   **ado_to_ao_shift** – flags whether the word ends in *-ao* (a common Cuban spelling reduction)

There are roughly 15 target rows in the table.\

An interesting observation is that several informal or colloquial spellings (like **pescao**, **melao**, **mantecao**, **cuidao**) appear alongside their standard forms (**pescado**, **mantecado**, **cuidado**). This highlights how Cuban Spanish often drops the intervocalic **/d/** in **-ado** endings during casual speech.

```{python}
import pandas as pd
import re, unicodedata
import spacy
import matplotlib.pyplot as plt

nlp = spacy.load("es_core_news_sm")

# 1. CLEAN & NORMALIZE
def clean_token(w):
    w = str(w).lower()
    w = re.sub(r'[^\wáéíóúñ]+', '', w)
    w = unicodedata.normalize('NFC', w)
    return w

cuban_words = [clean_token(w) for w in cuban_words if clean_token(w) != ""]

# 2. FREQUENCY DF
freq = {}
for w in cuban_words:
    freq[w] = freq.get(w, 0) + 1
df = pd.DataFrame(list(freq.items()), columns=["word", "frequency"])

# 3. MORPHEME LABELS
def label_morpheme(word):
    endings = {
        "-ao": r"ao$",
        "-aa": r"aa$",
        "-aos": r"aos$",
        "-aas": r"aas$",
        "-ado": r"ado$",
        "-ada": r"ada$",
        "-ados": r"ados$",
        "-adas": r"adas$"
    }
    for lbl, pat in endings.items():
        if re.search(pat, word):
            return lbl
    return None

df["morpheme"] = df["word"].apply(label_morpheme)
df_morph = df[df["morpheme"].notna()].copy()  # copy to avoid SettingWithCopyWarning

# 4. POS TAGGING (safe with .loc)
df_morph.loc[:, "pos"] = df_morph["word"].apply(lambda w: nlp(w)[0].pos_)

# 5. COUNT POS DISTRIBUTION
pos_distribution = pd.crosstab(df_morph["morpheme"], df_morph["pos"])
print("\n### TOKEN COUNT BY MORPHOLOGICAL ENDING ###")
print(df_morph["morpheme"].value_counts())

# 6. STACKED BAR PLOT
ax = pos_distribution.plot(
    kind="bar",
    stacked=True,
    figsize=(12, 7),
    title="POS Distribution for Standard vs. Reduced Endings in Cuban Spanish"
)
plt.xlabel("Word Ending")
plt.ylabel("Token Count")
plt.xticks(rotation=0)
plt.legend(title="POS Tag", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.savefig("pos.png", dpi=300, bbox_inches='tight')  # saves as PNG

plt.show()

```

```{python}
# ============================================================
# FREQUENCY OF REDUCED VS STANDARD FORMS (8 ENDINGS)
# ============================================================

import pandas as pd
import re, unicodedata
import matplotlib.pyplot as plt

# ----------------------------
# 1. CLEAN & NORMALIZE TOKENS
# ----------------------------
def clean_token(w):
    w = str(w).lower()
    w = re.sub(r'[^\wáéíóúñ]+', '', w)  # remove punctuation
    w = unicodedata.normalize('NFC', w)
    return w

cuban_words_clean = [clean_token(w) for w in cuban_words if clean_token(w) != ""]

# ----------------------------
# 2. DEFINE MORPHOLOGICAL ENDINGS
# ----------------------------
endings = {
    "-ao": r"ao$",
    "-aos": r"aos$",
    "-aa": r"aa$",
    "-aas": r"aas$",
    "-ado": r"ado$",
    "-ados": r"ados$",
    "-ada": r"ada$",
    "-adas": r"adas$"
}

# ----------------------------
# 3. COUNT FREQUENCIES
# ----------------------------
freq_counts = {}
for label, pattern in endings.items():
    freq_counts[label] = sum(1 for w in cuban_words_clean if re.search(pattern, w))

# Convert to DataFrame for plotting
freq_df = pd.DataFrame.from_dict(freq_counts, orient='index', columns=['Frequency'])

# ----------------------------
# 4. PLOT BAR CHART
# ----------------------------
colors = {
    "-ao": "#ff9999", "-aos": "#ff9999", "-aa": "#ff9999", "-aas": "#ff9999",  # reduced
    "-ado": "#66b3ff", "-ados": "#66b3ff", "-ada": "#66b3ff", "-adas": "#66b3ff"  # standard
}

ax = freq_df.plot(
    kind='bar',
    figsize=(10,6),
    color=[colors[idx] for idx in freq_df.index],
    legend=False,
    title="Frequency of Reduced vs Standard Forms in Cuban Spanish"
)

plt.ylabel("Token Count")
plt.xlabel("Word Ending")
plt.xticks(rotation=0)

# Add value labels on top of bars
for i, val in enumerate(freq_df['Frequency']):
    ax.text(i, val + 0.5, str(val), ha='center', va='bottom')

plt.tight_layout()
plt.savefig("frequency.png", dpi=300, bbox_inches='tight')  # saves as PNG
plt.show()

```

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import re
import unicodedata

# -------------------------------
# 0) ASSUMING cuban_words EXISTS
# -------------------------------
# If you already have cuban_words, skip this.
# If not, load your text file here:
# with open("YOURFILE.txt", "r", encoding="utf-8") as f:
#     cuban_words = f.read().split()

# -------------------------------
# 1) Build DF of token counts
# -------------------------------
def word_freq(words):
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    return freq

cuban_freq = word_freq(cuban_words)
df = pd.DataFrame(list(cuban_freq.items()), columns=["word", "frequency"])

# -------------------------------
# 2) Normalize Spanish tokens
# -------------------------------
def normalize_token(token):
    token = str(token).lower()
    token = re.sub(r"[^\wáéíóúñü]", "", token)  # remove punctuation
    token = unicodedata.normalize("NFC", token)  # normalize accents
    return token

df["norm_word"] = df["word"].apply(normalize_token)

# -------------------------------
# 3) Accent Minimal Pairs
# -------------------------------
accent_pairs = {
    "que": "qué",
    "el": "él",
    "tu": "tú",
    "mi": "mí",
    "si": "sí"
}

# -------------------------------
# 4) Count occurrences of each pair
# -------------------------------
pair_counts = []
for unaccented, accented in accent_pairs.items():
    unaccented_count = df[df["norm_word"] == unaccented]["frequency"].sum()
    accented_count = df[df["norm_word"] == accented]["frequency"].sum()
    pair_counts.append([unaccented + "/" + accented, unaccented_count, accented_count])

accent_df = pd.DataFrame(pair_counts, columns=["pair", "Unaccented", "Accented"])
accent_df.set_index("pair", inplace=True)

print("\n### Frequency Counts for Accent Minimal Pairs ###")
print(accent_df)

# -------------------------------
# 5) Plot Grouped Bar Chart
# -------------------------------
plt.figure(figsize=(10, 6))
ax = accent_df.plot(kind="bar", width=0.8, figsize=(10,6), color=["gray", "royalblue"])

# Add value labels above bars
for i, (idx, row) in enumerate(accent_df.iterrows()):
    ax.text(i - 0.18, row["Unaccented"] + 0.3, str(row["Unaccented"]), color="black")
    ax.text(i + 0.05, row["Accented"] + 0.3, str(row["Accented"]), color="black")

plt.title("Frequency of Accented vs. Unaccented Forms in Cuban Spanish", fontsize=13)
plt.ylabel("Token Count")
plt.xlabel("Minimal Pairs (Accent-driven Meaning Change)")
plt.xticks(rotation=0)
plt.legend(title="Form Type")
plt.tight_layout()

# SAVE to use in your HTML site
plt.savefig("accent_pair_frequencies.png", dpi=300, bbox_inches='tight')
plt.show()

```
